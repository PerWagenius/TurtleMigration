{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the GEBCO dataset\n",
    "ds = xr.open_dataset('C:/Users/18479/OneDrive/Desktop/Code Turtle Project/gebco_08.nc')\n",
    "\n",
    "##MAKE THIS BETTER VIA INTERPOLATION (NOT JUST THE CLOSEST GRIDPOINT)\n",
    "def get_bathymetry(lat,lon,ds):\n",
    "    # Get spacing and range from the dataset attributes\n",
    "    spacing = ds.spacing.values  # This will give you the spacing array,[0.00833333, 0.00833333]\n",
    "    x_range = ds.x_range.values  # This is the longitude range, [-180.0, 180.0]\n",
    "    y_range = ds.y_range.values  # This is the latitude range, [-90.0, 90.0]\n",
    "\n",
    "    # Calculate the index for latitude (y) and longitude (x)\n",
    "    lat_idx = int((y_range[1] - lat) / spacing[1])  # Reverse the calculation for latitude\n",
    "    lon_idx = int((lon - x_range[0]) / spacing[0])  # Longitude corresponds to x_range\n",
    "\n",
    "    # Convert 2D indices to 1D index\n",
    "    z_index = lat_idx * 43200  + lon_idx\n",
    "    #z_index = lat_idx  + lon_idx * 21600\n",
    "    # Get bathymetry value\n",
    "    bathymetry = ds.z[z_index].values\n",
    "    return bathymetry\n",
    "    print(f\"Latitude Index: {lat_idx}, Longitude Index: {lon_idx}\")\n",
    "    print(f\"Bathymetry Value: {bathymetry_value}\")\n",
    "    return bathymetry_value\n",
    "def addBath(file_path):\n",
    "    \"\"\"Add bathymetry data to a CSV file.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['bathymetry'] = df.apply(lambda row: get_bathymetry(row['latitude'], row['longitude'], ds), axis=1)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "TRACK_FILE = os.getenv('TRACK_FILE', 'path_to_track_file.csv')\n",
    "new_df = addBath(TRACK_FILE)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import MagneticModel as magmod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "MAGMODEL_PATH = os.getenv('MAGMODEL_PATH', 'path_to_magmodel_file.pkl')\n",
    "with open(MAGMODEL_PATH, 'rb') as file:\n",
    "    magmodel = pickle.load(file)\n",
    "\n",
    "def addIntInc(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Intensity'] = np.nan\n",
    "    df['Inclination']=np.nan\n",
    "    for i in range(0, len(df)):\n",
    "        _, _, _, _, _, df.loc[i, 'Intensity'],df.loc[i, 'Inclination']=magmodel.evaluate_model(df.loc[i, 'latitude'], df.loc[i, 'longitude'])\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "TRACK_FILE = os.getenv('TRACK_FILE', 'path_to_track_file.csv')\n",
    "new_df = addIntInc(TRACK_FILE)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_scrambled_path(df, pathnum, ds, magmodel):\n",
    "    # Starting location and time from the DataFrame\n",
    "    start_lat, start_lon, start_time = df['latitude'].iloc[0], df['longitude'].iloc[0], pd.to_datetime(df['datetime'].iloc[0])\n",
    "\n",
    "    # Shuffle delta columns as rows\n",
    "    scrambled_df = df[['deltaT', 'deltaLat', 'deltaLon']].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Initialize lists to store path coordinates and timestamps\n",
    "    lats = [start_lat]\n",
    "    lons = [start_lon]\n",
    "    times = [start_time]\n",
    "\n",
    "    # Generate path using scrambled delta values\n",
    "    for _, row in scrambled_df.iterrows():\n",
    "        # Compute new position\n",
    "        new_lat = lats[-1] + row['deltaLat']\n",
    "        new_lon = lons[-1] + row['deltaLon']\n",
    "        new_time = times[-1] + pd.to_timedelta(row['deltaT'], unit='s')\n",
    "\n",
    "        # Append to path lists\n",
    "        lats.append(new_lat)\n",
    "        lons.append(new_lon)\n",
    "        times.append(new_time)\n",
    "\n",
    "\n",
    "    # Prepare new columns for the scrambled path\n",
    "    scrambled_path_data = {\n",
    "        f\"lat_fakepath_{pathnum}\": lats[1:],\n",
    "        f\"lon_fakepath_{pathnum}\": lons[1:],\n",
    "        f\"time_fakepath_{pathnum}\": times[1:],\n",
    "        f\"bat_fakepath_{pathnum}\": [\n",
    "            get_bathymetry(lat, lon, ds) for lat, lon in zip(lats[1:], lons[1:])\n",
    "        ],\n",
    "        f\"int_fakepath_{pathnum}\": [np.nan] * len(lats[1:]),\n",
    "        f\"inc_fakepath_{pathnum}\": [np.nan] * len(lats[1:]),\n",
    "    }\n",
    "\n",
    "    # Apply magnetic model evaluation\n",
    "    for i in range(len(scrambled_path_data[f\"lat_fakepath_{pathnum}\"])):\n",
    "        lat = scrambled_path_data[f\"lat_fakepath_{pathnum}\"][i]\n",
    "        lon = scrambled_path_data[f\"lon_fakepath_{pathnum}\"][i]\n",
    "        _, _, _, _, _, scrambled_path_data[f\"int_fakepath_{pathnum}\"][i], scrambled_path_data[f\"inc_fakepath_{pathnum}\"][i] = magmodel.evaluate_model(lat, lon)\n",
    "\n",
    "    # Append the new columns to the DataFrame\n",
    "    new_columns_df = pd.DataFrame(scrambled_path_data)\n",
    "    return pd.concat([df, new_columns_df], axis=1)\n",
    "\n",
    "\n",
    "# Directory containing CSV files\n",
    "TRACKS_DIRECTORY = os.getenv('TRACKS_DIRECTORY', 'path_to_tracks_directory')\n",
    "\n",
    "for filename in os.listdir(TRACKS_DIRECTORY):\n",
    "    if filename.endswith(\".csv\") and filename.startswith(\"T\"):  # Process only relevant CSV files\n",
    "        file_path = os.path.join(TRACKS_DIRECTORY, filename)\n",
    "\n",
    "        # Load the real path\n",
    "        df = pd.read_csv(file_path, delimiter=',')\n",
    "        print(df.head())\n",
    "\n",
    "        for i in range(101):\n",
    "            df = add_scrambled_path(df, str(i), ds, magmodel)\n",
    "            print(\"made a new fake path\")\n",
    "\n",
    "        # Save a new CSV\n",
    "        output_file_path = os.path.join(TRACKS_DIRECTORY, f\"{os.path.splitext(filename)[0]}_fakepathsadded.csv\")\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Processed and saved: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing .si files\n",
    "FAKE_PATHS_DIRECTORY = os.getenv('FAKE_PATHS_DIRECTORY', 'path_to_fake_paths_directory')\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(FAKE_PATHS_DIRECTORY):\n",
    "    file_path = os.path.join(FAKE_PATHS_DIRECTORY, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.insert(3, 'inc_realpath',  np.nan)\n",
    "    df.insert(3,'int_realpath',  np.nan)\n",
    "    for i in range(0, len(df)):\n",
    "        _, _, _, _, _, df.loc[i, 'Intensity'],df.loc[i, 'Inclination']=magmodel.evaluate_model(df.loc[i, 'latitude'], df.loc[i, 'longitude'])\n",
    "    df.insert(3, 'bat_realpath', df.apply(lambda row: get_bathymetry(row['latitude'], row['longitude'], ds), axis=1))\n",
    "    df = df.drop(columns=['deltaT', 'deltaLat','deltaLon'])\n",
    "    df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "FAKE_PATHS_DIRECTORY = os.getenv('FAKE_PATHS_DIRECTORY', 'path_to_fake_paths_directory')\n",
    "for filename in os.listdir(FAKE_PATHS_DIRECTORY):\n",
    "    file_path = os.path.join(FAKE_PATHS_DIRECTORY, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(file_path)\n",
    "    print(df.isnull().values.any())\n",
    "    if 'Intensity' in df.columns:\n",
    "        df[\"int_realpath\"]=df['Intensity']\n",
    "        df[\"inc_realpath\"]=df['Inclination']\n",
    "        df = df.drop(columns=['Intensity', 'Inclination'])\n",
    "        df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
